diff --git a/framework/operators/fusion_ops/conv_batchnorm_scale_relu_pool.cpp b/framework/operators/fusion_ops/conv_batchnorm_scale_relu_pool.cpp
index b7df025..449fadb 100644
--- a/framework/operators/fusion_ops/conv_batchnorm_scale_relu_pool.cpp
+++ b/framework/operators/fusion_ops/conv_batchnorm_scale_relu_pool.cpp
@@ -4,19 +4,18 @@ namespace anakin {
 
 namespace ops {
 
-#ifdef USE_CUDA
-template<>
-void ConvBatchnormScaleReluPool<NV, AK_FLOAT, Precision::FP32>::operator() (OpContext<NV> &ctx, 
-                                                                            const std::vector<Tensor4dPtr<NV, AK_FLOAT> >& ins, 
-                                                                            std::vector<Tensor4dPtr<NV, AK_FLOAT> >& outs) {
-    auto* impl = static_cast<ConvBatchnormScaleReluPoolHelper<NV, AK_FLOAT, Precision::FP32>*>(this->_helper);
-    auto& param = static_cast<ConvBatchnormScaleReluPoolHelper<NV, AK_FLOAT, Precision::FP32>*>(this->_helper)->_param_conv_batchnorm_scale_relu_pooling;
-    impl->_funcs_conv_batchnorm_scale_relu_pooling(ins, outs, param, ctx);
+#define INSTANCE_CONVBATCHNORMSCALERELUPOOLING(Ttype, Dtype, Ptype) \
+template<> \
+void ConvBatchnormScaleReluPool<Ttype, Dtype, Ptype>::operator()(\
+    OpContext<Ttype>& ctx,\
+    const std::vector<Tensor4dPtr<Ttype, Dtype> >& ins,\
+    std::vector<Tensor4dPtr<Ttype, Dtype> >& outs) {\
+    auto* impl = static_cast<ConvBatchnormScaleReluPoolHelper<Ttype, Dtype, Ptype>*>\
+                 (this->_helper);\
+    auto& param = static_cast<ConvBatchnormScaleReluPoolHelper<Ttype, Dtype, Ptype>*>\
+                  (this->_helper)->_param_conv_batchnorm_scale_relu_pooling;\
+    SABER_CHECK(impl->_funcs_conv_batchnorm_scale_relu_pooling(ins, outs, param, ctx));\
 }
-#endif
-
-/// TODO ... specialization other type of operator
-
 
 /// set helper
 template<typename Ttype, DataType Dtype, Precision Ptype>
@@ -122,38 +121,38 @@ template<typename Ttype, DataType Dtype, Precision Ptype>
 Status ConvBatchnormScaleReluPoolHelper<Ttype, Dtype, Ptype>::Init(OpContext<Ttype> &ctx, 
                                                                    const std::vector<Tensor4dPtr<Ttype, Dtype> >& ins,
                                                                    std::vector<Tensor4dPtr<Ttype, Dtype> >& outs) {
-    _funcs_conv_batchnorm_scale_relu_pooling.init(ins, outs, _param_conv_batchnorm_scale_relu_pooling, SPECIFY, VENDER_IMPL, ctx);
+    SABER_CHECK(_funcs_conv_batchnorm_scale_relu_pooling.init(ins, outs, \
+        _param_conv_batchnorm_scale_relu_pooling, SPECIFY, SABER_IMPL, ctx));
     return Status::OK();
 }
 
 template<typename Ttype, DataType Dtype, Precision Ptype>
 Status ConvBatchnormScaleReluPoolHelper<Ttype, Dtype, Ptype>::InferShape(const std::vector<Tensor4dPtr<Ttype, Dtype> >& ins,
                                                                          std::vector<Tensor4dPtr<Ttype, Dtype> >& outs) {
-   _funcs_conv_batchnorm_scale_relu_pooling.compute_output_shape(ins, outs, _param_conv_batchnorm_scale_relu_pooling);
+    SABER_CHECK(_funcs_conv_batchnorm_scale_relu_pooling.compute_output_shape(ins, outs, \
+        _param_conv_batchnorm_scale_relu_pooling));
    return Status::OK();
 }
 
 #ifdef USE_CUDA
-template class ConvBatchnormScaleReluPoolHelper<NV, AK_FLOAT, Precision::FP32>;
-template class ConvBatchnormScaleReluPoolHelper<NV, AK_FLOAT, Precision::FP16>;
-template class ConvBatchnormScaleReluPoolHelper<NV, AK_FLOAT, Precision::INT8>;
+INSTANCE_CONVBATCHNORMSCALERELUPOOLING(NV, AK_FLOAT, Precision::FP32);
+template<>
+Status ConvBatchnormScaleReluPoolHelper<NV, AK_FLOAT, Precision::FP32>::Init(OpContext<NV> &ctx,
+                                                                   const std::vector<Tensor4dPtr<NV, AK_FLOAT> >& ins,
+                                                                   std::vector<Tensor4dPtr<NV, AK_FLOAT> >& outs) {
+    _funcs_conv_batchnorm_scale_relu_pooling.init(ins, outs, _param_conv_batchnorm_scale_relu_pooling, SPECIFY, VENDER_IMPL, ctx);
+    return Status::OK();
+}
+ANAKIN_REGISTER_OP_HELPER(ConvBatchnormScaleReluPool, ConvBatchnormScaleReluPoolHelper, NV, AK_FLOAT, Precision::FP32);
 #endif
 
 #ifdef USE_ARM_PLACE
+INSTANCE_CONVBATCHNORMSCALERELUPOOLING(ARM, AK_FLOAT, Precision::FP32);
 template class ConvBatchnormScaleReluPoolHelper<ARM, AK_FLOAT, Precision::FP32>;
-template class ConvBatchnormScaleReluPoolHelper<ARM, AK_FLOAT, Precision::FP16>;
-template class ConvBatchnormScaleReluPoolHelper<ARM, AK_FLOAT, Precision::INT8>;
-#endif
-
-// register helper 
-#ifdef USE_CUDA
-ANAKIN_REGISTER_OP_HELPER(ConvBatchnormScaleReluPool, ConvBatchnormScaleReluPoolHelper, NV, AK_FLOAT, Precision::FP32);
-#endif 
-
-#ifdef USE_ARM_PLACE
 ANAKIN_REGISTER_OP_HELPER(ConvBatchnormScaleReluPool, ConvBatchnormScaleReluPoolHelper, ARM, AK_FLOAT, Precision::FP32);
 #endif
 
+
 //! register op
 ANAKIN_REGISTER_OP(ConvBatchnormScaleReluPool)
     .Doc("ConvBatchnormScaleReluPool fusion operator")
